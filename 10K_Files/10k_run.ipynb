{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ticker                            Name                   Sector  \\\n",
      "0   INSM                      INSMED INC              Health Care   \n",
      "1   FTAI               FTAI AVIATION LTD              Industrials   \n",
      "2  XTSLA  BLK CSH FND TREASURY SL AGENCY  Cash and/or Derivatives   \n",
      "3     FN                        FABRINET   Information Technology   \n",
      "4   PCVX                     VAXCYTE INC              Health Care   \n",
      "\n",
      "    Asset Class  Market Value  Weight (%)  Notional Value          Shares  \\\n",
      "0        Equity  3.026505e+08        0.46  302,650,483.35    3,945,385.00   \n",
      "1        Equity  2.479806e+08        0.38  247,980,623.80    2,537,665.00   \n",
      "2  Money Market  2.355547e+08        0.36  235,554,705.69  235,554,706.00   \n",
      "3        Equity  2.329875e+08        0.36  232,987,503.36      926,318.00   \n",
      "4        Equity  2.290238e+08        0.35  229,023,767.28    2,779,752.00   \n",
      "\n",
      "    Price       Location                      Exchange Currency  FX Rate  \\\n",
      "0   76.71  United States                        NASDAQ      USD      1.0   \n",
      "1   97.72  United States                        NASDAQ      USD      1.0   \n",
      "2    1.00  United States                             -      USD      1.0   \n",
      "3  251.52  United States  New York Stock Exchange Inc.      USD      1.0   \n",
      "4   82.39  United States                        NASDAQ      USD      1.0   \n",
      "\n",
      "  Market Currency Accrual Date  \n",
      "0             USD            -  \n",
      "1             USD            -  \n",
      "2             USD            -  \n",
      "3             USD            -  \n",
      "4             USD            -  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Try loading the CSV file while skipping bad lines\n",
    "russell_unclean_df = pd.read_csv(\"split_russel.csv\", on_bad_lines='skip')\n",
    "\n",
    "# Remove commas from the 'Market Value' column and convert to numeric\n",
    "russell_unclean_df['Market Value'] = russell_unclean_df['Market Value'].str.replace(',', '')\n",
    "russell_unclean_df['Market Value'] = pd.to_numeric(russell_unclean_df['Market Value'], errors='coerce')\n",
    "\n",
    "# Sort in DESC order\n",
    "sorted_df = russell_unclean_df.sort_values(by='Market Value', ascending=False)\n",
    "print(sorted_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['JNJ', 'MCD', 'RTX']]\n"
     ]
    }
   ],
   "source": [
    "API_KEY = 'XXXX'\n",
    "\n",
    "# create batches of tickers: [[A,B,C], [D,E,F], ...]\n",
    "# a single batch has a maximum of max_length_of_batch tickers\n",
    "def create_batches(tickers = [], max_length_of_batch = 100):\n",
    "  batches = [[]]\n",
    "\n",
    "  for ticker in tickers:\n",
    "    if len(batches[len(batches)-1]) == max_length_of_batch:\n",
    "      batches.append([])\n",
    "\n",
    "    batches[len(batches)-1].append(ticker)\n",
    "\n",
    "  return batches\n",
    "\n",
    "batches = create_batches(list(sorted_df['Ticker']))\n",
    "print(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Starting downloading metadata for years 2015 to 2017\n",
      "✅ Downloaded and filtered metadata for batch ['JNJ', 'MCD', 'RTX']\n",
      "✅ Download completed. Metadata downloaded for 9 filings.\n"
     ]
    }
   ],
   "source": [
    "from sec_api import QueryApi\n",
    "import json\n",
    "import time\n",
    "\n",
    "queryApi = QueryApi(api_key=API_KEY)\n",
    "\n",
    "def download_10K_metadata(tickers=[], start_year=2015, end_year=2017):\n",
    "    print('✅ Starting downloading metadata for years {} to {}'.format(start_year, end_year))\n",
    "\n",
    "    batches = create_batches(tickers)\n",
    "    all_metadata = []\n",
    "\n",
    "    for batch in batches:  # Go through tickers\n",
    "        ticker_data = {}\n",
    "        for year in range(start_year, end_year + 1):  # Fulfill all years for each ticker\n",
    "            tickers_joined = ', '.join(batch)\n",
    "            ticker_query = 'ticker:({})'.format(tickers_joined)\n",
    "\n",
    "            query_string = '{ticker_query} AND filedAt:[{start_year}-01-01 TO {end_year}-12-31] AND formType:\"10-K\" AND NOT formType:\"10-K/A\" AND NOT formType:NT'.format(\n",
    "                ticker_query=ticker_query, start_year=year, end_year=year)\n",
    "\n",
    "            query = {\n",
    "                \"query\": query_string,\n",
    "                \"from\": \"0\",\n",
    "                \"size\": \"200\",\n",
    "                \"sort\": [{\"filedAt\": {\"order\": \"desc\"}}]\n",
    "            }\n",
    "\n",
    "            response = queryApi.get_filings(query)\n",
    "            filings = response['filings']  # Return values\n",
    "\n",
    "            for filing in filings:\n",
    "                ticker = filing['ticker']\n",
    "                if ticker not in ticker_data:\n",
    "                    ticker_data[ticker] = []\n",
    "\n",
    "                ticker_data[ticker].append({\n",
    "                    'ticker': ticker,\n",
    "                    'companyName': filing['companyName'],\n",
    "                    'cik': filing['cik'],\n",
    "                    'formType': filing['formType'],\n",
    "                    'year': year,\n",
    "                    'filedAt': filing['filedAt'],\n",
    "                    'filingUrl': filing['linkToFilingDetails'],\n",
    "                })\n",
    "\n",
    "            time.sleep(5)  # Delay before the next request\n",
    "\n",
    "        # Filter tickers with complete data for all specified years\n",
    "        for ticker, data in ticker_data.items():\n",
    "            years_present = {entry['year'] for entry in data}\n",
    "            if years_present == set(range(start_year, end_year + 1)):\n",
    "                all_metadata.extend(data)\n",
    "\n",
    "        print('✅ Downloaded and filtered metadata for batch', batch)\n",
    "\n",
    "    if all_metadata:\n",
    "        with open('additional_10k.json', 'w') as json_file:\n",
    "            json.dump(all_metadata, json_file, indent=4)\n",
    "        print('✅ Download completed. Metadata downloaded for {} filings.'.format(len(all_metadata)))\n",
    "        return all_metadata\n",
    "    else:\n",
    "        print('❌ No complete metadata found for the specified tickers and years.')\n",
    "\n",
    "\n",
    "#---------Run This------------_#\n",
    "tickers = list(sorted_df['Ticker'])\n",
    "metadata = download_10K_metadata(tickers=tickers, start_year=2015, end_year=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 658\n",
      "Number of unique tickers: 215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(658, 215)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#json stats\n",
    "import json\n",
    "\n",
    "def analyze_json_file(file_path):\n",
    "    # Load the JSON file\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    # Calculate the number of rows (length of the JSON list)\n",
    "    total_rows = len(data)\n",
    "    \n",
    "    # Find the number of unique tickers\n",
    "    unique_tickers = {entry['ticker'] for entry in data}\n",
    "    number_of_unique_tickers = len(unique_tickers)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Total number of rows: {total_rows}\")\n",
    "    print(f\"Number of unique tickers: {number_of_unique_tickers}\")\n",
    "\n",
    "    return total_rows, number_of_unique_tickers\n",
    "\n",
    "# Example usage\n",
    "json_file_path = 'russel_10k.json'\n",
    "analyze_json_file(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed MCD for year 2015\n",
      "Processed MCD for year 2016\n",
      "Processed MCD for year 2017\n",
      "Processed JNJ for year 2015\n",
      "Processed JNJ for year 2016\n",
      "Processed JNJ for year 2017\n",
      "Processed RTX for year 2015\n",
      "Processed RTX for year 2016\n",
      "Processed RTX for year 2017\n",
      "✅ JSON file successfully updated and saved as 'additional_10k_extracted.json'\n"
     ]
    }
   ],
   "source": [
    "from sec_api import ExtractorApi \n",
    "\n",
    "# Load the existing JSON file into a DataFrame\n",
    "with open(\"additional_10k.json\", 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    ticker_df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the SEC API with your API key\n",
    "extractorApi = ExtractorApi(api_key=API_KEY)\n",
    "\n",
    "# Create empty lists to store the FLS and item7 data\n",
    "fls_data = []\n",
    "item7_data = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in ticker_df.iterrows():\n",
    "    url = row[\"filingUrl\"]\n",
    "    \n",
    "    # Get the FLS (Item 1) section\n",
    "    fls = extractorApi.get_section(filing_url=url, section=\"1\", return_type=\"text\")\n",
    "    fls_data.append(fls)\n",
    "    \n",
    "    # Get the Item 7 section\n",
    "    item7 = extractorApi.get_section(filing_url=url, section=\"7\", return_type=\"text\")\n",
    "    item7_data.append(item7)\n",
    "\n",
    "    print(f\"Processed {row['ticker']} for year {row['year']}\")\n",
    "    \n",
    "    time.sleep(5) #add 5 sec delay for each filing\n",
    "\n",
    "# Add the new data as columns to the DataFrame\n",
    "ticker_df['FLS'] = fls_data\n",
    "ticker_df['item7'] = item7_data\n",
    "\n",
    "# Convert the updated DataFrame back to a JSON object\n",
    "updated_data = ticker_df.to_dict(orient='records')\n",
    "\n",
    "# Save the updated data back to a JSON file\n",
    "with open(\"additional_10k_extracted.json\", 'w') as json_file:\n",
    "    json.dump(updated_data, json_file, indent=4)\n",
    "\n",
    "print(\"✅ JSON file successfully updated and saved as 'additional_10k_extracted.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered JSON file with unique tickers created successfully.\n"
     ]
    }
   ],
   "source": [
    "#GRABBING TICKER AND EXCHANGE CODE DATA\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "with open('russel_10k.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Load the CSV data into a DataFrame\n",
    "csv_df = pd.read_csv('split_russel.csv')\n",
    "\n",
    "# Prepare a dictionary to hold unique tickers\n",
    "unique_tickers = {}\n",
    "\n",
    "# Iterate over each entry in the JSON data\n",
    "for entry in data:\n",
    "    ticker = entry['ticker']\n",
    "    company_name = entry['companyName']\n",
    "    \n",
    "    # Check if the ticker is already added\n",
    "    if ticker not in unique_tickers:\n",
    "        # Find the exchange for the ticker\n",
    "        exchange_row = csv_df[csv_df['Ticker'] == ticker]\n",
    "        \n",
    "        if not exchange_row.empty:\n",
    "            exchange = exchange_row.iloc[0]['Exchange']\n",
    "            \n",
    "            # Add the ticker to the unique_tickers dictionary\n",
    "            unique_tickers[ticker] = {\n",
    "                \"ticker\": ticker,\n",
    "                \"companyName\": company_name,\n",
    "                \"exchange\": exchange\n",
    "            }\n",
    "\n",
    "# Convert the dictionary values to a list\n",
    "filtered_data = list(unique_tickers.values())\n",
    "\n",
    "# Write the filtered data to a new JSON file\n",
    "with open('filtered_data.json', 'w') as outfile:\n",
    "    json.dump(filtered_data, outfile, indent=4)\n",
    "\n",
    "print(\"Filtered JSON file with unique tickers created successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
